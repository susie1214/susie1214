import logging
from src.models.model_manager import get_llm_model

logger = logging.getLogger(__name__)

def generate_response(prompt: str, user_id: str = "default", max_tokens: int = 256, temperature: float = 0.7, language: str = "ko"):
    """
    SOLAR-7B ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„± (í•œêµ­ì–´/ì˜ì–´ ì§€ì›)
    
    Args:
        prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
        user_id: ì‚¬ìš©ì ID
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        temperature: ìƒì„± ì˜¨ë„ (0.0 ~ 1.0)
        language: ì–¸ì–´ ('ko' ë˜ëŠ” 'en')
    
    Returns:
        dict: ìƒì„±ëœ ì‘ë‹µê³¼ ë©”íƒ€ë°ì´í„°
    """
    
    try:
        model = get_llm_model()
        
        if not model:
            # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
            logger.warning("LLM model not loaded, using fallback response")
            fallback_msg = "ì£„ì†¡í•©ë‹ˆë‹¤. ëª¨ë¸ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤." if language == "ko" else "Sorry, loading model. Please try again."
            return {
                'response': fallback_msg,
                'tokens_used': 0,
                'model': 'SOLAR-7B',
                'user_id': user_id,
                'language': language,
                'error': 'model_not_loaded'
            }
        
        # ì–¸ì–´ì— ë”°ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        if language == "ko":
            system_prompt = """ë‹¹ì‹ ì€ í”„ë¡œê·¸ë¨ ìš´ì˜ì„ ì§€ì›í•˜ëŠ” ì¹œì ˆí•œ ì•ˆë‚´ ì±—ë´‡ Poly-iì…ë‹ˆë‹¤.

## í”„ë¡œê·¸ë¨ ì •ë³´

### ğŸ“‹ ì¶œì„ ë° êµí†µë¹„ ì•ˆë‚´
- **ì¶œì„ë¹„**: 1ì¼ 3,300ì› (ì›” 6ë§Œ6ì²œì› í•œë„)
- **ì·¨ì•½ê³„ì¸µ ì¶œì„ë¹„**: 1ì¼ 1ë§Œì› (ì›” 20ë§Œì› í•œë„)
- **êµí†µë¹„**: 1ì¼ 2,500ì› (ì›” 5ë§Œì› í•œë„)
- **ì§€ê¸‰ì¡°ê±´**: ë‹¨ìœ„ê¸°ê°„ 1ê°œì›” ë™ì•ˆ ì¶œì„ë¥  80% ì´ìƒì´ì–´ì•¼ í•¨
- **ì§€ê¸‰ì‹œê¸°**: ë‹¤ìŒë‹¬ ì¤‘ìˆœê²½ ê°œì¸ê³„ì¢Œë¡œ ì…ê¸ˆ

### ğŸ“ ìˆ˜ì—… ìš´ì˜
- **ìˆ˜ì—… ì‹œì‘ì‹œê°„**: ì˜¤ì „ 9ì‹œ
- **ì¶œì„ì²´í¬**: êµìˆ˜ë‹˜ì´ ì§ì ‘ í™•ì¸

### ğŸ¢ ì‹œì„¤ ì•ˆë‚´
- **2ì¸µ**: ë„ì„œê´€ (í–‰ì •ì‹¤ í¬í•¨)
- **1ì¸µ**: ë„ì‹œë½ ì„­ì·¨ ê³µê°„ (êµ¬ë‚´ ì‹ë‹¹ ì—†ìŒ)
- **í¸ì˜ì‹œì„¤**: ëƒ‰ì¥ê³ , ì „ìë Œì§€, ì •ìˆ˜ê¸°

## ëŒ€ë‹µ ë°©ì‹
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€
- ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì •ë³´ë¥¼ ì •ë¦¬
- ì¶œì„ë¹„/êµí†µë¹„ ê´€ë ¨ ì§ˆë¬¸ ì‹œ êµ¬ì²´ì ì¸ ê¸ˆì•¡ê³¼ ì¡°ê±´ ëª…ì‹œ"""
            prefix = "ì‚¬ìš©ì: "
            suffix = "\në‹µë³€:"
        else:
            system_prompt = """You are Poly-i, a friendly program support chatbot.

## Program Information

### ğŸ“‹ Attendance & Transportation Allowance
- **Attendance**: 3,300 won/day (Max 66,000 won/month)
- **Low-income Attendance**: 10,000 won/day (Max 200,000 won/month)
- **Transportation**: 2,500 won/day (Max 50,000 won/month)
- **Requirement**: 80% or higher monthly attendance rate
- **Payment**: Mid-next month to personal account

### ğŸ“ Classes & Operations
- **Start Time**: 9:00 AM
- **Attendance Check**: Instructor verification

### ğŸ¢ Facilities
- **Floor 2**: Library (with Administration Office)
- **Floor 1**: Lunch Area (No cafeteria available)
- **Amenities**: Refrigerator, Microwave, Water purifier

## Response Style
- Provide accurate and helpful answers
- Use markdown format for clarity
- Specify exact amounts and conditions for allowance inquiries"""
            prefix = "User: "
            suffix = "\nResponse:"
        
        full_prompt = f"{system_prompt}\n\n{prefix}{prompt}{suffix}"
        
        # ëª¨ë¸ ì‹¤í–‰
        output = model(
            full_prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=0.95,
            top_k=50,
            repeat_penalty=1.1,
            echo=False
        )
        
        response_text = output['choices'][0]['text'].strip()
        tokens_used = output['usage']['completion_tokens']
        
        return {
            'response': response_text,
            'tokens_used': tokens_used,
            'model': 'SOLAR-7B',
            'user_id': user_id,
            'language': language
        }
        
    except Exception as e:
        logger.error(f"LLM Generation Error: {str(e)}")
        error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}" if language == "ko" else f"Error occurred: {str(e)}"
        return {
            'response': error_msg,
            'tokens_used': 0,
            'model': 'SOLAR-7B',
            'user_id': user_id,
            'language': language,
            'error': str(e)
        }

def create_system_prompt(intent: str = "general"):
    """
    ì˜ë„ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    
    Args:
        intent: ì‚¬ìš©ì ì˜ë„ (general, inquiry, complaint, feedback ë“±)
    
    Returns:
        str: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    """
    
    prompts = {
        "general": """ë‹¹ì‹ ì€ Poly-ië¼ëŠ” ì¹œì ˆí•œ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
- ëª…í™•í•˜ê³  ê°„ê²°í•œ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
- ëª¨ë¥´ëŠ” ê²ƒì€ ì†”ì§í•˜ê²Œ ì¸ì •í•˜ì„¸ìš”.""",
        
        "inquiry": """ë‹¹ì‹ ì€ ì œí’ˆ/ì„œë¹„ìŠ¤ ë¬¸ì˜ë¥¼ ë‹´ë‹¹í•˜ëŠ” ìƒë‹´ì›ì…ë‹ˆë‹¤.
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
- í•„ìš”í•˜ë©´ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ì„ ìœ„í•´ ëª…í™•í•œ ì§ˆë¬¸ì„ í•˜ì„¸ìš”.
- ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ íƒœë„ë¥¼ ìœ ì§€í•˜ì„¸ìš”.""",
        
        "complaint": """ë‹¹ì‹ ì€ ë¯¼ì› ì²˜ë¦¬ ë‹´ë‹¹ìì…ë‹ˆë‹¤.
- ì‚¬ìš©ìì˜ ë¶ˆë§Œì„ ê³µê°í•˜ëŠ” íƒœë„ë¡œ ê²½ì²­í•˜ì„¸ìš”.
- ë¬¸ì œë¥¼ ì´í•´í•˜ë ¤ê³  ë…¸ë ¥í•˜ì„¸ìš”.
- í•´ê²° ë°©ì•ˆì„ ì ê·¹ì ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”.
- ìƒí™©ì— ë”°ë¼ ì¸ê°„ ë‹´ë‹¹ìë¡œì˜ ì „í™˜ì„ ì œì•ˆí•˜ì„¸ìš”.""",
        
        "feedback": """ë‹¹ì‹ ì€ í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ëŠ” ë‹´ë‹¹ìì…ë‹ˆë‹¤.
- ì‚¬ìš©ìì˜ ì˜ê²¬ì„ ê°œë°©ì ìœ¼ë¡œ ë°›ì•„ë“¤ì´ì„¸ìš”.
- ëª…í™•í•œ í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ì„¸ìš”.
- ê°ì‚¬ì˜ ë§ˆìŒì„ í‘œí˜„í•˜ì„¸ìš”.""",
    }
    
    return prompts.get(intent, prompts["general"])
