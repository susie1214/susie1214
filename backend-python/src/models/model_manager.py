import torch
from sentence_transformers import SentenceTransformer
import os
from pathlib import Path

# ì „ì—­ ëª¨ë¸ ì €ì¥ì†Œ
_models = {}

def initialize_models():
    """ëª¨ë¸ ì´ˆê¸°í™” (ì„œë²„ ì‹œì‘ì‹œ í•œë²ˆë§Œ ì‹¤í–‰)"""
    global _models
    
    # LLM ëª¨ë¸ ì´ˆê¸°í™” (SOLAR-7B GGUF)
    print("ğŸ“¥ Loading SOLAR-7B Model...")
    try:
        from llama_cpp import Llama
        
        model_path = os.getenv(
            'LLM_MODEL_PATH',
            './models/solar-10.7b-instruct-v1.0.Q4_K_M.gguf'
        )
        
        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if not os.path.isabs(model_path):
            model_path = os.path.join(os.path.dirname(__file__), '..', '..', model_path)
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model not found at {model_path}")
        
        print(f"  Model path: {model_path}")
        print(f"  Model size: {os.path.getsize(model_path) / (1024**3):.2f} GB")
        
        _models['llm'] = Llama(
            model_path=model_path,
            n_gpu_layers=40,           # GPU ë ˆì´ì–´ ìˆ˜
            n_ctx=4096,                # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            max_tokens=512,            # ìµœëŒ€ ìƒì„± í† í°
            temperature=0.7,           # ì˜¨ë„ (ì°½ì˜ì„±)
            top_p=0.95,                # Top-p ìƒ˜í”Œë§
            verbose=False
        )
        print("âœ… SOLAR-7B Model loaded successfully")
        print(f"  GPU Available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"  GPU: {torch.cuda.get_device_name(0)}")
        
    except ImportError:
        print("âš ï¸  llama-cpp-python not installed. Install with: pip install llama-cpp-python")
        print("    Using mock model for development")
        _models['llm'] = None
    except FileNotFoundError as e:
        print(f"âŒ {e}")
        _models['llm'] = None
    except Exception as e:
        print(f"âŒ Failed to load SOLAR-7B Model: {e}")
        _models['llm'] = None
    
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    print("ğŸ“¥ Loading Embedding Model...")
    try:
        _models['embedding'] = SentenceTransformer(
            'sentence-transformers/all-MiniLM-L6-v2',
            cache_folder='./models/embeddings',
            local_files_only=False
        )
        print("âœ… Embedding Model loaded")
    except Exception as e:
        print(f"âš ï¸ Failed to load embedding model: {e}")
        print("   Retrying with local files only...")
        try:
            _models['embedding'] = SentenceTransformer(
                'all-MiniLM-L6-v2',
                local_files_only=True
            )
            print("âœ… Embedding Model loaded (offline)")
        except:
            print("âŒ Embedding model unavailable - using CPU inference only")
            _models['embedding'] = None

def get_llm_model():
    """LLM ëª¨ë¸ ë°˜í™˜"""
    return _models.get('llm')

def get_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ ë°˜í™˜"""
    return _models.get('embedding')

def is_gpu_available():
    """GPU ê°€ìš©ì„± í™•ì¸"""
    return torch.cuda.is_available()
